# Introduction to AWS EC2

In this lecture, we will learn about EC2 instances in AWS. Perhaps one of the most commonly used services from AWS or any cloud provider for that matter are virtual machines in the cloud. These virtual machines provide scalable compute that can be deployed in a matter of minutes. In AWS, they are called as EC2 instances, where EC2 stands for elastic compute cloud and just like any compute, virtual or physical, an EC2 instance would run an operating system such as a distribution of Linux or windows. We can then make use of these instances for deploying software such as database, web servers, application servers or pretty much anything that you would normally want to deploy in a physical or virtual machine, which is on premise. AWS EC2 provides pre-configured templates known as Amazon machine image or AMIs. These templates contain software configuration such as the operating system and any additional software to be deployed on these EC2 instances. Examples are Ubuntu 20.04, RHEL 8, Amazon Linux 2, windows 2019, et cetera. These AMIs have an ID which is specific to the region where we want to deploy the instance. EC2 also provides a number of different configurations of CPU, memory and networking capacity for the instances which are known as instance types. There are a wide selection of instance types to choose from that are optimized to fit different use cases. One example of the instance type is the general purpose instance type, and this can be used for several types of common workloads. Then there are compute optimized instance types to work on workloads that require high-performance CPUs such as batch processing workloads and data modeling. The memory optimized instance types are designed to deliver fast performance for workloads that process large data sets in memory. These are just few of the examples. For the complete list, please check out the reference documentation. The most commonly used instance type that suits most of the common workloads is the general purpose instance. These are further divided into different categories such as T2, T3, and M5 instances for example. Each of these categories can be sized to the specification that a user may need. For example, the T2 Nano provides one virtual CPU and half a gig of RAM. The T2 micro provides one vCPU and one gig of RAM. If you want to go for a higher spec, you can choose from a number of different sizes such as the small, medium, large, all the way up to 2X large. The sizes shown here are specific to the T2 general purpose instance type, and they may not necessarily be the same for all other instances. For a comprehensive list, please refer to the reference documentation. Persistent storage for these instances is provided by another service called the EBS, which stands for elastic block storage. As of this recording, there are five types of EBS volumes. Three of these are high-performance solid state type disks, and the other two are low cost hard disk drives. The size and type of storage to be attached to an EC2 instance is available for the user to select before provisioning. Additional disks may also be attached post provisioning. The EC2 also allows us to pass in user data to the instances being created. This allows us to perform common configuration tasks or even run scripts when the instances start. For example, if you want to install the NGINX package on an Ubuntu server when it's launched, we can pass in a shell script as the user data like this. For windows instances, we can also pass in a PowerShell or a batch script with the same effect. Once deployed, the Linux EC2 instances are usually accessed using SSH keys. For the windows instances, we can make use of the remote desktop or RDP along with the username and password. In the subsequent demo, we will see how to deploy a Linux EC2 instance using the AWS management console.

# Demo
In this demo, we will learn how to provision a Linux EC2-instance using the AWS Management Console. From the management console, to get to the EC to service, click on the Services tab on the top left. The EC2 service is located under the service group for Compute. Another way to locate the service is to make use of the service search bar from the dashboard. We can also launch it by clicking on the Launch a virtual machine link just under the Search bar. We are now inside the EC2 console. Please note that we are currently in the central region, which is Canada, and this is where we will be deploying this instance. To create an EC2 instance, click on the Launch instance button. This will open up the launch wizard. The first thing to do is to select an AMI, which is Amazon machine image. We are going to deploy a web server on Ubuntu, so select the AMI for Ubuntu 20.04. As you can see, this AMI is free tier eligible. Next, we have to select the instance type, so let's use t2 micro. In the next step, we can configure the instance. We will leave most of these to the default values. For the network, we will make use of the default VPC, which is automatically created for this region. Think of VPC as an isolated private network within the AWS infrastructure where we can deploy services. Within this VPC, we will also make use of the default value for the subnet. Since we want this instance to be a web server, we'll make use of the user data feature of EC2 to pass in a shell script to install NGINX. The user data field is under the Advanced Details. When ready, click on Next. This will take us to the storage section, we will leave this to be default value. There is a single 8GB GP2 disk, which is an SSD type disk that will be used to create the root partition. Click Next, and we can now add some tags to our instance. Let us add a tag called Name with the value of web server. Next, we can configure security group for this instance, we should be able to SSH to this machine. Let's create a new security group, and let's call it SSH Access. Let's also add a proper description for this group. Under the Type, you'll find a drop-down with several different types of firewall rules, but we are only interested in an inbound rule that will allow SSH access, so leave the default value as it is. Since the source is set to 0.0.0.0/0, this will mean that the SSH connection is open from the entire world. While this is not a recommended setting for a production server, it will do for this demo. Next, we can review the instance configuration. Once you're happy with it, click on Launch. As soon as we click it, we will get a pop-up notification to use an existing key pair or to create a new key pair. Since we do not have a key pair created in this region yet, we will want to create a new one. Let's call it Web, and once that's created, download it to the machine. This will be later used as an authentication mechanism to SSH to our instance. Next, click on Launch instance. This can take a few minutes, so please be patient. Click on View instances to check the status of this EC2 instance. We have one EC2 instance called Webserver which is in a running state. Click on it and we should be able to see all the details about this instance. In the Details section, we can see the public IP created for this instance. Let's copy it into the clipboard. We can also see information such as the VPC, subnet, the instance type, and the AMI ID that's used for creating this instance. In the Security tab, we can see the security group that we created. Under the Networking tab, we can see public IPv4 address as well as the private IPv4 address which is created for this instance. Under the Storage tab, we can see the EBS volume that is used by this instance. Now, let us use SSH to get to this instance from the terminal. Here I am inside the terminal of my local machine. To SSH into the instance we will use the SSH command and pass in the private key called Web that we downloaded from the console. The user for this instance is Ubuntu, and we'll make use of the public IP that we copied from the console. The first time you run this command, you may run into an error like this. This is because the private key file is open and does not have the correct permissions. To fix this, use the chmod command and change the permission to 400 keeping only read permission for the owner. If we run the command again, we should be able to SSH into the instance. Finally, let's see if NGNIX was installed and started on the server. To do this, let's run the command system CTL status NGNIX. As you can see, NGNIX was installed and it is in a running state. That was a quick demonstration video to show you how to launch an Ubuntu EC2 machine using the AWS Management Console and to install enable and start NGNIX on it using the user-data feature of EC2.

# AWS EC2 with Terraform
Now it's time to use Terraform to deploy EC2 instances. To do this we'll make use of the AWS instance type resource. Let's call the resource as web server. This resource expects two mandatory arguments, the AMI, which is the AMI ID in the US-West-1 region where we want to deploy the Ubuntu instance. The second is the instance type. We want to deploy a low spec instance with just one CPU and 1 GB of Ram. We will opt for a t2.micro general purpose instance type. Optionally, to identify the EC2 instance, we can add tags to it by making use of the tags argument which is a map. Here, we have added two tags in key value pairs. The first is the Name with the value of web server. The next is the description which has a value of an NGINX web server on Ubuntu. We can also pass in an entire bash shell script to be run when the instance is launched. To do this, we'll make use of the user data argument and the heredoc syntax, like this. We can now use the Terraform plan and then the apply command to create the web server instance. All right, so we now have the instance up and running, but how do we access it from our client machine. Since this is an Ubuntu EC2 instance, the logical answer would be for us to make use of an SSH client and then connect to the server, but what is the IP address of this machine and how do we allow SSH access to it? Which SSH key do we use to connect to it? We have not specified any of these in our Terraform configuration. As a result, by default, we cannot access this from our client machine. In the demo where we created an EC2 instance from the management console, we created an SSH key pair just before launching the instance and we used this to connect to our server. Now, let's see how to do that with Terraform. Let's go back and update our Terraform configuration. To do this, we'll make use of another resource type called AWS key pair. This resource makes use of an existing user supplied key pair that can be used to control the login access to the EC2 instance. There's only one mandatory argument called the public key and here, we are making use of the file function to read the contents of an existing public key called web.pub which is stored in the local machine running Terraform. Alternatively, we can also provide the contents of the web.pub without using the file function, like this. Next, we can specify this key within the AWS resource block by making use of the key name argument like this. Now that we have added the key based access control to our configuration, let us look at the networking that will allow users to connect from the local machine to the port 22 on our web server via the internet. In our demo, when we we deployed the webserver manually, we saw that we used the default VPC and one of the available subnets for the EC2 instance. Think of VPC as an isolated network within your AWS infrastructure. Within this isolated network, we can deploy services. With the VPC, we have the complete control over the IP address range that can be assigned to a resource such as an EC2 instance. We can also allow and restrict access to this resource from other resources within the AWS cloud or even the internet. In the demo, we also created a new security group called SSH access, which as the name suggests, allows SSH access from any source by making use of the 0.0.0/0 range. We then used the security group while creating the web server instance. Although this is not a recommended approach in a production scenario, this configuration will allow us to access the Ubuntu webserver from the internet as long as we have the private key. Now let's do the same with our Terraform configuration. We will deploy this instance in the default VPC and subnet as in the demo, but we will create a security group that will allow ingress access to port 22 from the internet. For this, we have to make use of another resource type called AWS security group. Let's call this resource as SSH access. The name and description are optional, but we will give it some meaningful values such as SSH access for the name and a suitable description. Now that we have added the resource block for the security group, we have to apply it within the resource block for the EC2 instance. To do this, we'll make use of the argument called VPC security group IDs and use a reference expression to specify the ID of the security group that we just configured. This argument expects a list. Make sure that the values are supplied within square brackets like this. While we are at it, let's add an output variable to get the public IP address of the webserver instance. We can use this later to SSH to the webserver from our local machine. If we re-run the Terraform apply command now, we will see that the key pairs and the security groups are created and the EC2 instance is recreated because of the change in its configuration. Once the resources are created, we should be able to SSH to the webserver instance using the private key like this.


# Terraform Provisioners

Let us now recap provisioners in Terraform. Provisioners provide a way for us to carry out tasks such as running commands or scripts on remote resources or locally on the machine where Terraform is installed. For example, to run a bash script after a resource is created, we can make use of the remote exec provisioner. Using the same web server example that we have seen many times so far, we can pass in an inline script like this. As you can see from this example, the provisional block goes directly inside a resource block. These commands in the inline list will now be run on the remote instance after it is deployed. In this example, we are running an app to update, installing, enabling, and starting engine X on the EC2 instance after it is deployed. However, keep a note that nearly specifying the provisional block with the script will not guarantee that the commands will work. For it to work, there should be network connectivity between the local machine and the remote instance. For Linux instances, this means an SSH connection and if it's Windows, WINRM connectivity should be established. Now this can be achieved by making use of appropriate security groups while creating the remote resources. In this case, we are creating the security group resource, but it could also be an existing security group that's already available in the AWS account. There should also be authentication mechanism that can be used to connect to these instances. Most commonly an SSH key pair that can be used to access the instance. Again, in this example, we are using an SSH key pair in the resource definition. Now this key is created by the AWS keeper resource, but it could also be an existing key pair, which is available in the account. To facilitate the authentication, we can make use of the connection block like this. This is an example of a Linux based AMI as such the connection type is set to SSH. The host argument is where we specify the public IP address of the instance that was created and since we are operating directly inside the resource block, we can make use of an expression, self.public_ip like this. This will effectively translate into the public IP address of the instance that was provisioned. The user in this case is Ubuntu as that's the default user created within this AMI and the key we use to connect to is the private key. Remember that earlier we used the public key pair as an argument while creating the resource. Now, when we run Terraform apply, we should see the remote exec provisioner connecting to the instance using the details of the connection block and completing the task specified in the inline list. Next, let's take a look at the local exec provisioner. Local exec provisioner is used to run tasks on the local machine where we are running the Terraform binary and not on the resources which are created by Terraform. This is especially handy when we want to gather some data and write into a file locally. For example, if you want to store the public IP address of the EC 2 instance, that was just provisioned into a file called IP.text in the slash [?] file system, we can run the local exec provisioner like this and just like we saw with the remote exec provisioner, the provisional block for local exec also goes inside the resource block. Once the resource is created after a Terraform apply, we can see the public IP address recorded into the file. By default, provisioners are run after the resources are created. This is the default behavior, and it is called as a create time provisioner. We can also make provisioner run before a resource is destroyed. This is called a destroyed time provisioner. Now this can be specifically set by making use of the event argument and specifying its value as destroyed within the provisional block. Another default behavior of provisioner is that if the command or script within the provisional block fails, the Terraform apply operation also errors out. In this example, an incorrect part using the command causes the script to fail and results in an error. This is the default provisional behavior, but it can also be set by specifically mentioning on failure argument inside the provisional block like this. Any resource that is created while the provisional fails is marked as tainted within Terraform. For the Terraform apply operation to not fail and the resource to be created successfully even if the provisioner command of script fails, we can set the value of the on failure argument to continue like this. As the best practice, Terraform recommends to use provisionals as a last resort, wherever possible, make use of options natively available for resource types for the provider used. For example, use user data while creating an EC2 two instance in AWS and custom data for Azure virtual machines, metadata for GCP, et cetera.

# Provisioner Behaviour

In this lecture, we will see the different ways we can use provisioners in Terraform. In the previous lecture, the remote exec and the local-exec provisioners we used, run tasks when a resource is created. This is the default behavior and is known as the creation-time provisioner. Let's use the same example of the local-exec provisioner that we used in the previous lecture. When applied, the provisioner will record the public IP address of the webserver on the local machine. We can also make the provisioner run before a resource is destroyed. This is called a destroy-time provisioner. This can be specifically set by making use of the event argument as specifying its value as destroy, like this. Another default behavior of a provisional block is to cause a Terraform apply to fail if the tasks specified in it fails for any reason. For example, if the creation-time provisioner, that we used earlier, makes use of an incorrect path in the command, the default behavior would be for the Terraform apply to fail with an error, like this. Here, there is no /temp directory located in the local machine, and this results in a failure. The same behavior can be specified by adding an on_failure argument in the provisional block with a value of fail, like this. This is the default value, so it's not really required to be set. Please note, that when this happens, Terraform marks the resource as tainted. We have a lecture on resource taint in the following section. We may not always want the apply step to fail if the provisioner fails. For example, let's say, the provisioner script is optional and we don't care if it fails, we want the apply step to ignore this error and proceed. To override the default behavior and for apply to go through successfully, use the on_failure argument with "continue" as the value. When we run Terraform apply now, the resource will be provisioned successfully. Well, yhat brings us to the end of this lecture. Head over to the Hands-On Labs and practice working with Terraform provisioners.

# Considerations with Provisioners

Let us now look at some of the considerations to keep in mind while making use of provisioners. We learned how to use provisioners in Terraform in the previous lecture. While they are great for running tasks such as a bootstrap script using remote exec, Terraform recommends to use provisioners sparingly, and this is because of a couple of reasons. Making use of provisioners increase the complexity of the configuration, and since we can, basically, run any system supported command within the command or inline argument, Terraform plan has no way to accurately model the actions of a provisioner. Secondly, as we saw in the example of remote exec, a connection block must be defined for some provisioners to work. This means that the network connectivity from the local machine and the authentication must be established before the provisioner is run. And this may not always be desirable or feasible. In order to avoid this, Terraform recommends to make use of provisioners that are native to the resource. We already saw an example of this when we use user data while creating the EC2 instance for Terraform. User data is a native feature of AWS EC2, and it runs during the instance launch time without having to define the connection block. The user data argument is specific to the AWS EC2 resource, but there are similar arguments for other cloud providers that can be used in the same way, instead of using the remote-exec provisioner. Some of these options are listed here. As a best practice, it is recommended to keep the post provisioning tasks to a minimum and build custom images that will have all the software and configurations required for a resource, beforehand. Following the same example as before, instead of installing NGINX during launch time, using user data or remote-exec provisioner, make sure that we use a custom AMI of Ubuntu which already has NGINX installed. And this is where templating tools come into the picture. We can build custom AMIs, either from an existing instance that has all the software and configuration installed, or make use of tools like Packer which can do this in a declarative way. Once this image is built, we can use it to build an AWS Instance, and this time, there is no need to use user data for remote-exec provisioner to install NGINX.